{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6a47162",
   "metadata": {},
   "source": [
    "#### Classification\n",
    "This notebook applies a Random Forest classification model to delineate wetland extent using the preprocessed raster data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3c92648",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import rasterio     \n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "from collections import Counter\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from rasterio.transform import rowcol\n",
    "from rasterio.windows import from_bounds\n",
    "from rasterio.warp import reproject, Resampling\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "from skimage.morphology import opening, square\n",
    "from scipy.interpolate import griddata\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d25f3f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base directory found: C:\\Users\\Ethel Ogallo\\Documents\\ZFL1\\Data\n"
     ]
    }
   ],
   "source": [
    "# Base directory where data is stored\n",
    "base_dir = r\"C:\\Users\\Ethel Ogallo\\Documents\\ZFL1\\Data\"\n",
    "raster_dir = r\"C:\\Users\\Ethel Ogallo\\Documents\\ZFL1\\Data\\processed_data\\cropped_rasters\"\n",
    "\n",
    "# Verify the directory exists\n",
    "if not os.path.exists(base_dir):\n",
    "    print(f\"WARNING: Base directory does not exist: {base_dir}\\nPlease check your path!\")\n",
    "else:\n",
    "    print(f\"Base directory found: {base_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6783a85",
   "metadata": {},
   "source": [
    "#### Loading Training Points\n",
    "Importing and preparing the training point data for use in classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c10bccf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed extent: (35.9167, 0.3, 36.216699999999996, 0.8999999999999999)\n",
      "    id    class                  geometry\n",
      "0    1    water  POINT (36.06711 0.56802)\n",
      "1    2    water  POINT (36.04937 0.67043)\n",
      "2    3    water  POINT (36.09725 0.47869)\n",
      "3    4    water  POINT (36.09869 0.47104)\n",
      "4    5    water  POINT (36.07187 0.34713)\n",
      "..  ..      ...                       ...\n",
      "70  70  wetland  POINT (36.08588 0.46504)\n",
      "71  71  wetland  POINT (36.08948 0.46191)\n",
      "72  72  wetland  POINT (36.09104 0.46122)\n",
      "73  73  wetland  POINT (36.08973 0.49504)\n",
      "74  74  wetland  POINT (36.07059 0.50154)\n",
      "\n",
      "[75 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define AOI extents\n",
    "# Central point in decimal degrees\n",
    "center_lat = 0.6\n",
    "center_lon = 36.0667\n",
    "\n",
    "# Define a bounding box: ±0.3° latitude (north-south), ±0.15° longitude (east-west)\n",
    "lat_buffer = 0.3\n",
    "lon_buffer = 0.15\n",
    "\n",
    "# Compute extent (bounding box)\n",
    "extents = (\n",
    "    center_lon - lon_buffer,  # xmin (left)\n",
    "    center_lat - lat_buffer,  # ymin (bottom)\n",
    "    center_lon + lon_buffer,  # xmax (right)\n",
    "    center_lat + lat_buffer   # ymax (top)\n",
    ")\n",
    "\n",
    "print(\"Computed extent:\", extents)\n",
    "\n",
    "# Load training points and reproject to match LST raster CRS if needed\n",
    "gdf = gpd.read_file(os.path.join(base_dir, 'baringo_training_points.shp'))\n",
    "with rasterio.open(os.path.join(base_dir, \"input/DEM\", \"DEM.tif\")) as src:\n",
    "    dem_crs = src.crs\n",
    "if gdf.crs != dem_crs:\n",
    "    gdf = gdf.to_crs(dem_crs)\n",
    "\n",
    "#gdf.head()\n",
    "print(gdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a14d755",
   "metadata": {},
   "source": [
    "#### Stacking All Raster Data\n",
    "Combining all individual raster layers into a single multi-band raster stack, ensuring all layers are properly aligned and clipped to the area of interest (AOI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf80f86c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2018...\n",
      "Missing layer 'NDWI_SD' for 2018\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2018\n",
      "Missing layer 'relativeDEM' for 2018\n",
      "Missing layer 'Inundation_freq' for 2018\n",
      "Year 2018: Stacked 2 layers -> stack shape: (2222, 1113, 2)\n",
      "Processing year 2019...\n",
      "Missing layer 'NDWI_SD' for 2019\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2019\n",
      "Missing layer 'relativeDEM' for 2019\n",
      "Missing layer 'Inundation_freq' for 2019\n",
      "Year 2019: Stacked 2 layers -> stack shape: (2222, 1113, 2)\n",
      "Processing year 2020...\n",
      "Missing layer 'NDWI_SD' for 2020\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2020\n",
      "Missing layer 'relativeDEM' for 2020\n",
      "Missing layer 'Inundation_freq' for 2020\n",
      "Year 2020: Stacked 2 layers -> stack shape: (2222, 1113, 2)\n",
      "Processing year 2021...\n",
      "Missing layer 'NDWI_SD' for 2021\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2021\n",
      "Missing layer 'relativeDEM' for 2021\n",
      "Missing layer 'Inundation_freq' for 2021\n",
      "Year 2021: Stacked 2 layers -> stack shape: (2222, 1113, 2)\n",
      "Processing year 2022...\n",
      "Missing layer 'NDWI_SD' for 2022\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2022\n",
      "Missing layer 'relativeDEM' for 2022\n",
      "Missing layer 'Inundation_freq' for 2022\n",
      "Year 2022: Stacked 2 layers -> stack shape: (2222, 1113, 2)\n",
      "Processing year 2023...\n",
      "Missing layer 'NDWI_SD' for 2023\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2023\n",
      "Missing layer 'relativeDEM' for 2023\n",
      "Missing layer 'Inundation_freq' for 2023\n",
      "Year 2023: Stacked 2 layers -> stack shape: (2222, 1113, 2)\n",
      "Processing year 2024...\n",
      "Missing layer 'NDVI_SDNDVI_range' for 2024\n",
      "Missing layer 'Inundation_freq' for 2024\n",
      "Year 2024: Stacked 4 layers -> stack shape: (2222, 1113, 4)\n"
     ]
    }
   ],
   "source": [
    "temporal_names = [\"LST\", \"NDWI_SD\", \"NDVI_SD\" \"NDVI_range\"]\n",
    "static_names = [\"relativeDEM\", \"TWI\", \"Inundation\"]\n",
    "\n",
    "years = range(2018, 2025)  # inclusive of 2024\n",
    "feature_stack_by_year = {}\n",
    "\n",
    "# Function to load + resample\n",
    "def load_and_resample(path, ref_shape, ref_transform, ref_crs):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read(1).astype(float)\n",
    "        nodata = src.nodata\n",
    "        if nodata is not None:\n",
    "            arr[arr == nodata] = np.nan  # convert nodata to NaN\n",
    "\n",
    "        if src.shape != ref_shape or src.crs != ref_crs or src.transform != ref_transform:\n",
    "            dst = np.empty(ref_shape, dtype=arr.dtype)\n",
    "            reproject(\n",
    "                source=arr,\n",
    "                destination=dst,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=ref_transform,\n",
    "                dst_crs=ref_crs,\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            arr = dst\n",
    "    return arr\n",
    "\n",
    "# Loop over all years\n",
    "for year in years:\n",
    "    print(f\"Processing year {year}...\")\n",
    "\n",
    "    # Reference raster (DEM for that year)\n",
    "    ref_path = os.path.join(raster_dir, str(year), \"DEM.tif\")\n",
    "    if not os.path.exists(ref_path):\n",
    "        print(f\"Reference DEM missing for {year}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    with rasterio.open(ref_path) as ref_src:\n",
    "        ref_shape = ref_src.shape\n",
    "        ref_transform = ref_src.transform\n",
    "        ref_crs = ref_src.crs\n",
    "\n",
    "    layers, layer_names = [], []\n",
    "\n",
    "    for name in temporal_names + static_names:\n",
    "        path = os.path.join(raster_dir, str(year), f\"{name}.tif\")\n",
    "        if os.path.exists(path):\n",
    "            arr = load_and_resample(path, ref_shape, ref_transform, ref_crs)\n",
    "            layers.append(arr)\n",
    "            layer_names.append(name)\n",
    "        else:\n",
    "            print(f\"Missing layer '{name}' for {year}\")\n",
    "\n",
    "    if layers:\n",
    "        stack = np.stack(layers, axis=-1)\n",
    "        feature_stack_by_year[year] = {\"stack\": stack, \"layer_names\": layer_names}\n",
    "        print(f\"Year {year}: Stacked {len(layers)} layers -> stack shape: {stack.shape}\")\n",
    "    else:\n",
    "        print(f\"No layers stacked for {year}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876dca2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "temporal_names = [\"LST\", \"NDWI_SD\", \"NDVI_SD\", \"NDWI_Range\", \"NDVI_range\"]\n",
    "static_names = [\"relativeDEM\", \"TWI\", \"Inundation_v2\"]\n",
    "\n",
    "year = range(2018, 2025)\n",
    "feature_stack_by_year = {}\n",
    "\n",
    "# Reference raster\n",
    "ref_path = os.path.join(raster_dir, str(year), \"DEM.tif\")\n",
    "with rasterio.open(ref_path) as ref_src:\n",
    "    ref_shape = ref_src.shape\n",
    "    ref_transform = ref_src.transform\n",
    "    ref_crs = ref_src.crs\n",
    "    ref_meta = ref_src.meta.copy()\n",
    "\n",
    "# Load + resample function\n",
    "def load_and_resample(path, ref_shape, ref_transform, ref_crs):\n",
    "    with rasterio.open(path) as src:\n",
    "        arr = src.read(1)\n",
    "        if src.shape != ref_shape or src.crs != ref_crs or src.transform != ref_transform:\n",
    "            dst = np.empty(ref_shape, dtype=arr.dtype)\n",
    "            reproject(\n",
    "                source=arr,\n",
    "                destination=dst,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=ref_transform,\n",
    "                dst_crs=ref_crs,\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            arr = dst\n",
    "    return arr\n",
    "\n",
    "# Load + stack with nodata handling\n",
    "# Load + stack with correct nodata handling\n",
    "layers, layer_names = [], []\n",
    "\n",
    "for name in temporal_names + static_names:\n",
    "    path = os.path.join(raster_dir, str(year), f\"{name}.tif\")\n",
    "    if os.path.exists(path):\n",
    "        with rasterio.open(path) as src:\n",
    "            arr = src.read(1).astype(float)\n",
    "            nodata = src.nodata\n",
    "            if nodata is not None:\n",
    "                arr[arr == nodata] = np.nan  # convert -9999 -> NaN\n",
    "\n",
    "        # Now resample after nodata conversion\n",
    "        if arr.shape != ref_shape or src.crs != ref_crs or src.transform != ref_transform:\n",
    "            dst = np.empty(ref_shape, dtype=arr.dtype)\n",
    "            reproject(\n",
    "                source=arr,\n",
    "                destination=dst,\n",
    "                src_transform=src.transform,\n",
    "                src_crs=src.crs,\n",
    "                dst_transform=ref_transform,\n",
    "                dst_crs=ref_crs,\n",
    "                resampling=Resampling.bilinear\n",
    "            )\n",
    "            arr = dst\n",
    "\n",
    "        layers.append(arr)\n",
    "        layer_names.append(name)\n",
    "    else:\n",
    "        print(f\"Missing layer '{name}'\")\n",
    "\n",
    "\n",
    "# Stack\n",
    "stack = np.stack(layers, axis=-1)\n",
    "feature_stack_by_year[year] = {\"stack\": stack, \"layer_names\": layer_names}\n",
    "print(f\"Year {year}: Stacked {len(layers)} layers -> stack shape: {stack.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea747190",
   "metadata": {},
   "outputs": [],
   "source": [
    "stack = feature_stack_by_year[year][\"stack\"]\n",
    "layer_names = feature_stack_by_year[year][\"layer_names\"]\n",
    "\n",
    "# Assign colormaps\n",
    "colormap_map = {\n",
    "    \"LST\": \"inferno\",\n",
    "    \"NDWI_SD\": \"Blues\",\n",
    "    \"NDVI_SD\": \"Greens\",\n",
    "    \"NDWI_Range\": \"PuBu\",\n",
    "    \"NDVI_range\": \"YlGn\",\n",
    "    \"relativeDEM\": \"terrain\",\n",
    "    \"TWI\": \"cividis\",\n",
    "    \"Inundation_v2\": \"Blues\"\n",
    "}\n",
    "\n",
    "n_layers = len(layer_names)\n",
    "n_cols = math.ceil(n_layers / 2)\n",
    "n_rows = 2\n",
    "\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(4 * n_cols, 4 * n_rows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(n_layers):\n",
    "    data = stack[:, :, i]\n",
    "    \n",
    "    # Mask NaN values so they don't appear\n",
    "    masked_data = np.ma.masked_invalid(data)\n",
    "    \n",
    "    cmap = colormap_map.get(layer_names[i], \"viridis\")\n",
    "    img = axes[i].imshow(masked_data, cmap=cmap)\n",
    "    axes[i].set_title(layer_names[i], fontsize=10)\n",
    "    axes[i].axis('off')\n",
    "    fig.colorbar(img, ax=axes[i], fraction=0.046, pad=0.04)\n",
    "\n",
    "# Hide extra subplots if any\n",
    "for j in range(n_layers, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4983ac",
   "metadata": {},
   "source": [
    "##### Extracting Pixel Values at Training Points\n",
    "Sampling raster feature values at the locations of training points to create the input data for model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c79ac71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get transform of cropped DEM once, since extent is the same for all years\n",
    "dem_path = os.path.join(raster_dir, \"2024\", \"DEM.tif\")\n",
    "with rasterio.open(dem_path) as src:\n",
    "    window = from_bounds(*extents, transform=src.transform)\n",
    "    cropped_transform = src.window_transform(window)\n",
    "\n",
    "# Prepare list of (x, y) coordinates from the training points GeoDataFrame\n",
    "coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "# Initialize dict \n",
    "if 'training_data_by_year' not in globals():\n",
    "    training_data_by_year = {}\n",
    "\n",
    "for year, data in feature_stack_by_year.items():\n",
    "    feature_stack = data['stack']\n",
    "    transform = cropped_transform\n",
    "\n",
    "    # Convert (x, y) coordinates to raster (row, col)\n",
    "    rows_cols = [rowcol(transform, x, y) for x, y in coords]\n",
    "\n",
    "    pixels, labels = [], []\n",
    "    for (row, col), label in zip(rows_cols, gdf[\"class\"]):\n",
    "        # Check bounds\n",
    "        if 0 <= row < feature_stack.shape[0] and 0 <= col < feature_stack.shape[1]:\n",
    "            px_values = feature_stack[row, col, :]\n",
    "            # Ignore if any value is NaN\n",
    "            if not np.any(np.isnan(px_values)):\n",
    "                pixels.append(px_values)\n",
    "                labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(pixels)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Store\n",
    "    training_data_by_year[year] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y_encoded,\n",
    "        \"label_encoder\": le\n",
    "    }\n",
    "\n",
    "    print(f\"Year {year}: extracted {len(y)} samples with {X.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba6dd31a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## for 2024 only\n",
    "\n",
    "# Prepare list of (x, y) coordinates from the training points GeoDataFrame\n",
    "coords = [(geom.x, geom.y) for geom in gdf.geometry]\n",
    "\n",
    "# Initialize dict \n",
    "if 'training_data_by_year' not in globals():\n",
    "    training_data_by_year = {}\n",
    "\n",
    "for year, data in feature_stack_by_year.items():\n",
    "    feature_stack = data['stack']\n",
    "    transform = ref_transform  \n",
    "\n",
    "    pixels, labels = [], []\n",
    "    for (x, y), label in zip(coords, gdf[\"class\"]):\n",
    "        row, col = rowcol(transform, x, y)\n",
    "        # Check bounds\n",
    "        if 0 <= row < feature_stack.shape[0] and 0 <= col < feature_stack.shape[1]:\n",
    "            px_values = feature_stack[row, col, :]\n",
    "            # Ignore if any value is NaN\n",
    "            if not np.any(np.isnan(px_values)):\n",
    "                pixels.append(px_values)\n",
    "                labels.append(label)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(pixels)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    if len(X) == 0:\n",
    "        print(f\"Year {year}: No valid training points inside raster!\")\n",
    "        continue\n",
    "\n",
    "    # Encode labels\n",
    "    le = LabelEncoder()\n",
    "    y_encoded = le.fit_transform(y)\n",
    "\n",
    "    # Store\n",
    "    training_data_by_year[year] = {\n",
    "        \"X\": X,\n",
    "        \"y\": y_encoded,\n",
    "        \"label_encoder\": le\n",
    "    }\n",
    "\n",
    "    print(f\"Year {year}: extracted {len(y)} samples with {X.shape[1]} features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b2289e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "df_samples = pd.DataFrame(training_data_by_year[year]['X'], columns=layer_names)\n",
    "df_samples['class'] = training_data_by_year[year]['y']\n",
    "df_samples.iloc[:-10]\n",
    "#df_samples.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58f468",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = df_samples['class'].value_counts()\n",
    "print(\"Number of training points per class:\")\n",
    "print(class_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6a4949",
   "metadata": {},
   "source": [
    "#### Stratified 5-Fold Cross-Validation with Random Forest\n",
    "This section covers training the Random Forest classifier using the prepared training dataset.\n",
    "The stratified 5-fold cross-validation is applied to evaluate model performance, ensuring each fold preserves the class distribution.\n",
    "\n",
    "The workflow includes both combined train/test splitting and cross-validation to optimize and validate the classification model. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167ac561",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_per_year(year, training_data, test_size=0.2, n_estimators_cv=100, n_estimators_final=200, random_state=42):\n",
    "    \"\"\"\n",
    "    Train Random Forest model for each year with train/test split and cross-validation.\n",
    "    \n",
    "    Returns:\n",
    "        model: final trained model on combined train+val\n",
    "        metrics: dict with CV and test accuracies and predictions\n",
    "    \"\"\"\n",
    "    data = training_data[year]\n",
    "    X, y, le = data[\"X\"], data[\"y\"], data[\"label_encoder\"]\n",
    "\n",
    "    from collections import Counter\n",
    "    class_counts = Counter(y)\n",
    "    min_class_count = min(class_counts.values())\n",
    "\n",
    "    if min_class_count < 2:\n",
    "        print(f\"Warning: Year {year} has class with less than 2 samples. Skipping stratified split and CV.\")\n",
    "\n",
    "        # Train final model on all data (no split)\n",
    "        final_rf = RandomForestClassifier(n_estimators=n_estimators_final, random_state=random_state)\n",
    "        final_rf.fit(X, y)\n",
    "\n",
    "        metrics = {\n",
    "            \"cv_true\": None,\n",
    "            \"cv_pred\": None,\n",
    "            \"cv_accuracy\": None,\n",
    "            \"y_test\": None,\n",
    "            \"y_test_pred\": None,\n",
    "            \"test_accuracy\": None,\n",
    "            \"label_encoder\": le,\n",
    "        }\n",
    "        return final_rf, metrics\n",
    "\n",
    "    # Proceed with stratified split since all classes have >= 2 samples\n",
    "    X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, stratify=y, random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Cross-validation training\n",
    "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=random_state)\n",
    "    cv_true, cv_pred = [], []\n",
    "    for train_idx, val_idx in skf.split(X_train_val, y_train_val):\n",
    "        rf = RandomForestClassifier(n_estimators=n_estimators_cv, random_state=random_state)\n",
    "        rf.fit(X_train_val[train_idx], y_train_val[train_idx])\n",
    "        y_val_pred = rf.predict(X_train_val[val_idx])\n",
    "        cv_true.extend(y_train_val[val_idx])\n",
    "        cv_pred.extend(y_val_pred)\n",
    "\n",
    "    cv_accuracy = accuracy_score(cv_true, cv_pred)\n",
    "\n",
    "    # Final model on combined train+val\n",
    "    final_rf = RandomForestClassifier(n_estimators=n_estimators_final, random_state=random_state)\n",
    "    final_rf.fit(X_train_val, y_train_val)\n",
    "\n",
    "    # Test set evaluation\n",
    "    y_test_pred = final_rf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "    metrics = {\n",
    "        \"cv_true\": cv_true,\n",
    "        \"cv_pred\": cv_pred,\n",
    "        \"cv_accuracy\": cv_accuracy,\n",
    "        \"y_test\": y_test,\n",
    "        \"y_test_pred\": y_test_pred,\n",
    "        \"test_accuracy\": test_accuracy,\n",
    "        \"label_encoder\": le,\n",
    "    }\n",
    "\n",
    "    return final_rf, metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b72e7f",
   "metadata": {},
   "source": [
    "\n",
    "Using the trained Random Forest model, the wetland classes on the study area is predicted.\n",
    "Following classification, the area covered by each wetland class is estimated in hectares (Ha)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880418f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model for a specific year\n",
    "def prediction_per_year(year, model, feature_stack_by_year, extent, label_encoder):\n",
    "    \"\"\"\n",
    "    Predict classification raster for one year and compute area per class.\n",
    "    \n",
    "    Returns:\n",
    "        pred_img: 2D numpy array of predictions with -1 outside AOI\n",
    "        class_areas: dict of class names to area in hectares\n",
    "    \"\"\"\n",
    "    feature_stack = feature_stack_by_year[year][\"stack\"]\n",
    "    flat = feature_stack.reshape(-1, feature_stack.shape[-1])\n",
    "    valid_mask = ~np.isnan(flat).any(axis=1)\n",
    "\n",
    "    pred = np.full(flat.shape[0], -1, dtype=int)\n",
    "    pred[valid_mask] = model.predict(flat[valid_mask])\n",
    "    pred_img = pred.reshape(feature_stack.shape[:2])\n",
    "\n",
    "    counts = Counter(pred_img[pred_img >= 0])\n",
    "\n",
    "    # Calculate pixel area in hectares\n",
    "    lat = (extent[1] + extent[3]) / 2\n",
    "    deg_to_m = 111320  # meters per degree\n",
    "    pixel_w = abs(extent[2] - extent[0]) / pred_img.shape[1]\n",
    "    pixel_h = abs(extent[3] - extent[1]) / pred_img.shape[0]\n",
    "    pixel_area_ha = (pixel_w * deg_to_m * math.cos(math.radians(lat))) * (pixel_h * deg_to_m) / 10_000\n",
    "\n",
    "    class_areas = {\n",
    "        label_encoder.inverse_transform([cls])[0]: count * pixel_area_ha for cls, count in counts.items()\n",
    "    }\n",
    "\n",
    "    return pred_img, class_areas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f480736",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_by_year = {}\n",
    "metrics_by_year = {}\n",
    "predictions_by_year = {}\n",
    "\n",
    "for year in sorted(training_data_by_year.keys()):\n",
    "    print(f\"\\nProcessing year {year}...\")\n",
    "\n",
    "    # Train model\n",
    "    model, metrics = train_model_per_year(year, training_data_by_year)\n",
    "    models_by_year[year] = model\n",
    "    metrics_by_year[year] = metrics\n",
    "\n",
    "    print(f\"Year {year} CV Accuracy: {metrics['cv_accuracy']:.4f}\")\n",
    "    print(f\"Year {year} Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "\n",
    "    # Predict\n",
    "    pred_img, class_areas = prediction_per_year(year, model, feature_stack_by_year, extent, metrics[\"label_encoder\"])\n",
    "    predictions_by_year[year] = {\n",
    "        \"pred_img\": pred_img,\n",
    "        \"class_areas\": class_areas,\n",
    "    }\n",
    "\n",
    "    print(f\"Prediction complete for year {year}.\")          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122257f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "models_by_year = {}\n",
    "metrics_by_year = {}\n",
    "predictions_by_year = {}\n",
    "\n",
    "print(f\"\\nProcessing year {year}...\")\n",
    "\n",
    "# Train model\n",
    "model, metrics = train_model_per_year(year, training_data_by_year)\n",
    "models_by_year[year] = model\n",
    "metrics_by_year[year] = metrics\n",
    "\n",
    "print(f\"Year {year} CV Accuracy: {metrics['cv_accuracy']:.4f}\")\n",
    "print(f\"Year {year} Test Accuracy: {metrics['test_accuracy']:.4f}\")\n",
    "\n",
    "# Predict\n",
    "pred_img, class_areas = prediction_per_year(\n",
    "    year, \n",
    "    model, \n",
    "    feature_stack_by_year, \n",
    "    extents, \n",
    "    metrics[\"label_encoder\"]\n",
    ")\n",
    "predictions_by_year[year] = {\n",
    "    \"pred_img\": pred_img,\n",
    "    \"class_areas\": class_areas,\n",
    "}\n",
    "\n",
    "print(f\"Prediction complete for year {year}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72ce2462",
   "metadata": {},
   "source": [
    "#### Confusion Matrix\n",
    "Evaluating classification accuracy by comparing predicted classes against true labels using a confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8a2da",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024  # change as needed\n",
    "\n",
    "# The trained RF model\n",
    "rf_model = models_by_year[year]\n",
    "\n",
    "# Define feature names (matching the actual feature stack order)\n",
    "features = ['LST', 'NDWI_SD','NDWI_Range', 'NDVI_SD','NDVI_range', 'TWI','Inundation_v2', 'relativeDEM'] \n",
    "\n",
    "importances = (rf_model.feature_importances_) * 100\n",
    "idx = importances.argsort()[::-1]\n",
    "\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.bar(np.array(features)[idx], importances[idx], color='steelblue')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.title(f\"Feature Importances - {year}\")\n",
    "plt.ylabel(\"Importance (%)\")\n",
    "plt.xlabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# model test results metrics\n",
    "le = metrics_by_year[year]['label_encoder']\n",
    "y_test = metrics_by_year[year]['y_test']\n",
    "y_test_pred = metrics_by_year[year]['y_test_pred']\n",
    "\n",
    "print(f\"\\n=== {year} Test set Classification Report ===\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=le.classes_))\n",
    "\n",
    "# Plot confusion matrix\n",
    "#cv_true = metrics_by_year[year]['cv_true']\n",
    "#cv_pred = metrics_by_year[year]['cv_pred']\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "plt.figure(figsize=(4, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=le.classes_, yticklabels=le.classes_)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title(f'Confusion Matrix for test set - {year}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n=== Estimated area per class for year {year}:\")\n",
    "for cls, area in predictions_by_year[year]['class_areas'].items():\n",
    "    print(f\"  {cls}: {area:.2f} ha\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bf99113",
   "metadata": {},
   "source": [
    "#### Visualization of Yearly classification\n",
    "Displaying the predicted wetland classification results for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c63b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "years = sorted(predictions_by_year.keys())  \n",
    "n_years = len(years)\n",
    "\n",
    "# Setup colormap and norm \n",
    "le = metrics_by_year[years[0]]['label_encoder']\n",
    "labels = le.classes_\n",
    "cmap = ListedColormap(['#145a32', '#abb2b9', '#3498db', '#f39c12'][:len(labels)])\n",
    "norm = BoundaryNorm(np.arange(len(labels)+1)-0.5, cmap.N)\n",
    "xmin, ymin, xmax, ymax = extents\n",
    "\n",
    "\n",
    "# Define subplot grid size (2 rows, adjust columns)\n",
    "nrows = 2\n",
    "ncols = (n_years + 1) // 2\n",
    "\n",
    "fig, axes = plt.subplots(nrows, ncols, figsize=(6*ncols, 7*nrows))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, year in zip(axes, years):\n",
    "    pred_img = predictions_by_year[year]['pred_img']\n",
    "\n",
    "    im = ax.imshow(pred_img, cmap=cmap, norm=norm, \n",
    "                   extent=[xmin, xmax, ymin, ymax], origin='upper')\n",
    "    ax.set_title(f\"Classification Prediction for {year}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "    # Add colorbar per subplot\n",
    "    cbar = fig.colorbar(im, ax=ax, ticks=np.arange(len(labels)))\n",
    "    cbar.ax.set_yticklabels(labels)\n",
    "    cbar.set_label(\"Class\")\n",
    "\n",
    "# Remove empty subplots if any\n",
    "for ax in axes[n_years:]:\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98a8677",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2024\n",
    "pred_img = predictions_by_year[year]['pred_img']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8b20136",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import array_bounds\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 7))\n",
    "\n",
    "# Setup colormap and norm \n",
    "le = metrics_by_year[year]['label_encoder']\n",
    "labels = le.classes_\n",
    "cmap = ListedColormap(['#145a32', '#abb2b9', '#3498db', '#f39c12'][:len(labels)])\n",
    "norm = BoundaryNorm(np.arange(len(labels)+1)-0.5, cmap.N)\n",
    "\n",
    "# Compute extent from raster shape and ref_transform\n",
    "height, width = pred_img.shape\n",
    "xmin, ymin, xmax, ymax = array_bounds(height, width, ref_transform)\n",
    "extent = [xmin, xmax, ymin, ymax]\n",
    "\n",
    "im = ax.imshow(pred_img, cmap=cmap, norm=norm, \n",
    "               extent=extent, origin='upper')\n",
    "ax.set_title(f\"Classification Prediction for {year}\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "cbar = fig.colorbar(im, ax=ax, ticks=np.arange(len(labels)))\n",
    "cbar.ax.set_yticklabels(labels)\n",
    "cbar.set_label(\"Class\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9774ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the predictions for the year 2024\n",
    "out_tif = f\"C:\\\\Users\\\\Ethel Ogallo\\\\Documents\\\\ZFL1\\\\Data\\\\classification_{year}.tif\"\n",
    " \n",
    "\n",
    "with rasterio.open(\n",
    "    out_tif, 'w', driver='GTiff',\n",
    "    height=pred_img.shape[0], width=pred_img.shape[1],\n",
    "    count=1, dtype=pred_img.dtype,\n",
    "    crs=dem_crs, transform=cropped_transform\n",
    ") as dst:\n",
    "    dst.write(pred_img, 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f4598c",
   "metadata": {},
   "source": [
    "#### CART"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f7dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier, export_text, DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Get training data for 2024\n",
    "X = training_data_by_year[2024][\"X\"]\n",
    "y = training_data_by_year[2024][\"y\"]\n",
    "le = training_data_by_year[2024][\"label_encoder\"]\n",
    "\n",
    "\n",
    "# Split train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# CART model\n",
    "cart = DecisionTreeClassifier(\n",
    "    criterion='gini',\n",
    "    max_depth=None,\n",
    "    min_samples_leaf=1,\n",
    "    #class_weight='balanced',  \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "#cart =DecisionTreeClassifier()\n",
    "\n",
    "# Cross-validation\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(cart, X_train, y_train, cv=cv, scoring='accuracy')\n",
    "print(f\"5-Fold CV Accuracy: {cv_scores}\")\n",
    "print(f\"Mean CV Accuracy: {cv_scores.mean():.3f}\")\n",
    "\n",
    "# Train on full training set\n",
    "cart.fit(X_train, y_train)\n",
    "y_pred = cart.predict(X_test)\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred, labels=np.unique(y))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=le.classes_)\n",
    "disp.plot(cmap='Blues', values_format='d')\n",
    "plt.title(\"CART Confusion Matrix - 2024\")\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=le.classes_))\n",
    "\n",
    "# feature importance\n",
    "importances = cart.feature_importances_\n",
    "layer_names = feature_stack_by_year[2024][\"layer_names\"]\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "plt.figure(figsize=(6,6))\n",
    "plt.barh(np.array(layer_names)[indices], importances[indices], color='steelblue')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title(\"CART Feature Importance - 2024\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: print tree structure\n",
    "print(export_text(cart, feature_names=layer_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c08f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import array_bounds\n",
    "\n",
    "# Year\n",
    "year = 2024\n",
    "\n",
    "# Get the stack and transform\n",
    "feature_stack = feature_stack_by_year[year]['stack']\n",
    "layer_names = feature_stack_by_year[year]['layer_names']\n",
    "n_rows, n_cols, n_features = feature_stack.shape\n",
    "\n",
    "# Flatten for prediction\n",
    "flat_pixels = feature_stack.reshape(-1, n_features)\n",
    "\n",
    "# Mask NaNs\n",
    "mask = ~np.any(np.isnan(flat_pixels), axis=1)\n",
    "\n",
    "# Predict using CART\n",
    "flat_pred = np.full(flat_pixels.shape[0], fill_value=-1, dtype=int)\n",
    "flat_pred[mask] = cart.predict(flat_pixels[mask])\n",
    "\n",
    "# Ensure predictions match LabelEncoder indices\n",
    "le = training_data_by_year[year]['label_encoder']\n",
    "flat_pred_corrected = np.full_like(flat_pred, -1)\n",
    "flat_pred_corrected[mask] = le.transform(le.inverse_transform(flat_pred[mask]))\n",
    "\n",
    "# Reshape to raster\n",
    "pred_img = flat_pred_corrected.reshape(n_rows, n_cols)\n",
    "\n",
    "# Build colormap tied to class labels\n",
    "class_colors = {\n",
    "    'forest': '#145a32',     # dark green\n",
    "    'rangeland': '#abb2b9',  # gray\n",
    "    'water': '#3498db',      # blue\n",
    "    'wetland': '#f39c12'     # orange\n",
    "}\n",
    "cmap_list = [class_colors[cls] for cls in le.classes_]\n",
    "cmap = ListedColormap(cmap_list)\n",
    "norm = BoundaryNorm(np.arange(len(le.classes_)+1)-0.5, cmap.N)\n",
    "\n",
    "# Compute extent properly\n",
    "xmin, ymin, xmax, ymax = array_bounds(n_rows, n_cols, ref_transform)\n",
    "extent = [xmin, xmax, ymin, ymax]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "im = ax.imshow(pred_img, cmap=cmap, norm=norm, extent=extent, origin='upper')\n",
    "cbar = fig.colorbar(im, ax=ax, ticks=np.arange(len(le.classes_)))\n",
    "cbar.ax.set_yticklabels(le.classes_)\n",
    "cbar.set_label(\"Class\")\n",
    "ax.set_title(f\"Wetland Extent CART\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_aspect('equal')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128533d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.features import shapes\n",
    "from shapely.geometry import shape\n",
    "from shapely.ops import unary_union\n",
    "from scipy.ndimage import binary_dilation\n",
    "\n",
    "# Identify pixels of interest\n",
    "aoi_classes = ['water', 'wetland']\n",
    "aoi_indices = le.transform(aoi_classes)\n",
    "aoi_mask = np.isin(pred_img, aoi_indices)\n",
    "\n",
    "# using dilation to merge nearby patches: includes small non-water/wetland gaps inside the main AOI\n",
    "dilated_mask = binary_dilation(aoi_mask, iterations=6)  \n",
    "\n",
    "# Extract polygons\n",
    "polygons_generator = shapes(dilated_mask.astype(np.uint8), mask=dilated_mask, transform=ref_transform)\n",
    "polygons = [shape(geom) for geom, value in polygons_generator if value == 1]\n",
    "\n",
    "# Merge polygons into one main AOI\n",
    "aoi_gdf = gpd.GeoDataFrame(geometry=polygons, crs=\"EPSG:4326\")\n",
    "\n",
    "# Save AOI\n",
    "aoi_gdf.to_file(os.path.join(base_dir, 'wetland_extent.shp'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb749d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "from rasterio.transform import array_bounds\n",
    "\n",
    "# Build colormap for CART prediction\n",
    "class_colors = {\n",
    "    'forest': '#145a32',     # dark green\n",
    "    'rangeland': '#abb2b9',  # gray\n",
    "    'water': '#3498db',      # blue\n",
    "    'wetland': '#f39c12'     # orange\n",
    "}\n",
    "cmap_list = [class_colors[cls] for cls in le.classes_]\n",
    "cmap = ListedColormap(cmap_list)\n",
    "norm = BoundaryNorm(np.arange(len(le.classes_)+1)-0.5, cmap.N)\n",
    "\n",
    "# Compute raster extent\n",
    "xmin, ymin, xmax, ymax = array_bounds(n_rows, n_cols, ref_transform)\n",
    "extent = [xmin, xmax, ymin, ymax]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Classification raster\n",
    "im = ax.imshow(pred_img, cmap=cmap, norm=norm, extent=extent, origin='upper')\n",
    "\n",
    "# Overlay AOI polygons\n",
    "aoi_gdf2 = gpd.read_file(os.path.join(base_dir, 'wetland_extent_2024.shp'))\n",
    "aoi_gdf2.boundary.plot(ax=ax, color='red', linewidth=1)\n",
    "\n",
    "# Colorbar\n",
    "cbar = fig.colorbar(im, ax=ax, ticks=np.arange(len(le.classes_)))\n",
    "cbar.ax.set_yticklabels(le.classes_)\n",
    "cbar.set_label(\"Class\")\n",
    "\n",
    "ax.set_title(f\"Extent (water + wetland)\")\n",
    "ax.set_xlabel(\"Longitude\")\n",
    "ax.set_ylabel(\"Latitude\")\n",
    "ax.set_aspect('equal')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zfl_wetland",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
